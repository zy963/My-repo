---
title: "Lab_08T"
author: "36-600"
date: "Fall 2022"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

## Data

We'll begin by simulating a dataset from a nonlinear curve:
```{r}
set.seed(555)
x = -5:5
y = 0.1*x^3 - 0.5*x + 2.1 + rnorm(length(x),mean=0,sd=0.5*(1+abs(x)))
e = 0.5*(1+abs(x))

df = data.frame("x"=x,"y"=y,"e"=e)

suppressMessages(library(tidyverse))
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") +
  geom_point(color="firebrick")
```

# Questions

## Question 1

Assume that you know that a cubic polynomial is the correct function to assume here, but do not assume you know what the true parameters are. (And that means you should include the $x^2$ term in your curve-fitting analysis!) Code an optimizer that will allow you to estimate the four coefficient terms of a cubic polynomial. Try not to let the true cubic polynomial coefficients above influence your initial guesses. Show the coefficients and plot your result. Make sure your plot looks good before you move on to Question 2: with four terms, it can be relatively easy to find a locally optimal result that is not the globally optimal result, i.e., it can be relatively easy to find a local minimum in $\chi^2$ that is not the global minimum. It's pretty easy to identify when this is the case when you plot functions...so always plot them! 
```{r}
fit.fun <- function(par,data)
{
  pred = par[1]*(data$x)^3+par[2]*(data$x)^2+par[3]*(data$x)+par[4]
return( sum((data$y-pred)^2/(data$e)^2) ) # chi-square, coded
}
par <- c(0.1, 0.5, 0.6, 0.7)                      # initial guess
op.out <- suppressWarnings(optim(par,fit.fun,data=df))
op.out$value ; op.out$par

mod.x <- -5:5 ; mod.y <- op.out$par[1]*(x)^3+op.out$par[2]*(x)^2+op.out$par[3]*(x)+op.out$par[4]
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") + geom_point(color="firebrick") +
  geom_line(mapping=aes(x=mod.x,y=mod.y),color="olivedrab")
```

## Question 2

Take the minimum $\chi^2$ value that you found in Question 1 (it's in the output from `optim()`, even if you didn't explicitly display it) and perform a $\chi^2$ goodness of fit test. Recall that the null hypothesis is that the model is an acceptable one, i.e., that it plausibly replicates the data-generating process. Do you reject the null, or fail to reject the null? (Also recall that the number of degrees of freedom is $n-p$, where $n$ is the length of $x$ and $p$ is the number of coefficients in the cubic polynomial.)
```{r}
1 - pchisq(op.out$value,nrow(df)-1)
```
```
the p-value is above 0.05, which means we can accept the null hypothesis. Our model is acceptable.
```

## Question 3

Now let's assume you don't know the functional form, and you just want to find a plausible nonparametric function. Utilize `loess()` to do this. Play with the `span` parameter until you feel that the model is a good one, in your eyes. Recall that you will want something "smooth" that does not appear to be so flexible as to fit to the noise; try to find the smallest value of `span` that produces a plausibly smooth curve. (However: keep in mind that your `loess()` function is only defined at the $x$ points, so the output will appear to be piecewise [linear connecting the dots] even when it is truly "smooth." Call us over if you have no idea what we just said.) Plot the data and overlay the function, and indicate your `span` value.
```{r}
loess.out <- loess(y~x,data=df,weights=1/(df$e)^2,span=0.55)
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") +
  geom_point(color="firebrick") +
  geom_line(mapping=aes(x=loess.out$x,y=loess.out$fitted),color="olivedrab")
```
```
Span is 0.55.
```

## Question 4

Redo the plot in Question 3, but let's add a one-standard-error confidence band. You can do this by running the first line, then adding the last two lines onto your `ggplot()` call:
```
p = predict(loess.out,se=TRUE)

+ geom_line(mapping=aes(x=loess.out$x,y=loess.out$fitted+p$se),color="[your color]",linetype="dashed")
+ geom_line(mapping=aes(x=loess.out$x,y=loess.out$fitted-p$se),color="[your color]",linetype="dashed")
```
What does the band actually mean? Because it's a one-standard-error band, it means that for any given $x$, there is an approximately 68% chance that the band overlaps the true underlying function value. This is a rough statement, though, given the correlation between neighboring data points (i.e., the lack of independence between $y_{i-1}$, $y_i$, and $y_{i+1}$, etc.). Just think of the band as a notion of how uncertain your fitted curve is at each $x$: is the band thin, or wide? Note that the bands get wider as we get to either end of the data: this is an expected feature, not a bug. There's fewer data within the span at either end, so the fitted function is that much more uncertain.
```{r}
p = predict(loess.out,se=TRUE)

loess.out <- loess(y~x,data=df,weights=1/(df$e)^2,span=0.55)
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") +
  geom_point(color="firebrick") +
  geom_line(mapping=aes(x=loess.out$x,y=loess.out$fitted),color="olivedrab")+
  geom_line(mapping=aes(x=loess.out$x,y=loess.out$fitted+p$se),color="darkolivegreen",linetype="dashed")+
  geom_line(mapping=aes(x=loess.out$x,y=loess.out$fitted-p$se),color="darkolivegreen",linetype="dashed")
```
